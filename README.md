# Projet HDFS - Fusion de Petits Fichiers

## üìã Description du Projet

Ce projet impl√©mente une solution orient√©e objets pour r√©soudre le **probl√®me des petits fichiers** dans les syst√®mes distribu√©s de type HDFS (Hadoop Distributed File System), **bas√© sur l'article de recherche** *"Merging Small Files Based on Agglomerative Hierarchical Clustering on HDFS for Cloud Storage"*.

### Le Probl√®me des Petits Fichiers

Dans HDFS, chaque fichier g√©n√®re des m√©tadonn√©es stock√©es en m√©moire par le NameNode. Un grand nombre de petits fichiers entra√Æne:
- **Surcharge m√©moire** du NameNode
- **Performances d√©grad√©es** lors des op√©rations de lecture/√©criture
- **Co√ªt √©lev√©** de gestion des m√©tadonn√©es

### La Solution (Selon Article de Recherche)

Ce programme regroupe les petits fichiers en **clusters** en utilisant un algorithme de **clustering hi√©rarchique agglom√©ratif** avec la m√©thode **single-linkage**, permettant de:
- R√©duire le nombre de fichiers de m√©tadonn√©es
- Minimiser la surcharge du NameNode
- Optimiser l'utilisation du stockage

**üìå Configuration HDFS (selon article):**
- **Taille de bloc HDFS**: 128 MB
- **Seuil de petits fichiers**: **0.75 (75%)** de la taille de bloc
- **Taille maximale d'un petit fichier**: **96 MB** (75% √ó 128 MB)
- **Validation des acc√®s**: V√©rification de la taille lors de l'acc√®s aux fichiers

> *"The default threshold for this system is set to (0.75) 75% of default block size (128 MB). If the user accesses the files, the system will check the size of file."* - Article de recherche

---

## üßÆ M√©thode de Clustering Utilis√©e

### Algorithme: Clustering Hi√©rarchique Agglom√©ratif

**Principe (selon article):**
0. **Filtrage**: Identifier les petits fichiers (taille < 96 MB = 75% √ó 128 MB)
1. **Initialisation**: Chaque petit fichier commence comme un cluster individuel
2. **It√©ration**: √Ä chaque √©tape:
   - Trouver les deux clusters les plus proches
   - V√©rifier la contrainte de taille: `taille_totale ‚â§ 128 MB`
   - Si OK ‚Üí fusionner les clusters
   - Sinon ‚Üí marquer comme non fusionnable
3. **Terminaison**: Quand aucune fusion n'est plus possible

**Important**: Les fichiers ‚â• 96 MB ne sont **pas** trait√©s par le syst√®me de fusion et sont g√©r√©s directement par HDFS.

### Distance Entre Fichiers

La distance est calcul√©e **uniquement sur la taille**:

```
distance(fichier_i, fichier_j) = |taille_i - taille_j|
```

### M√©thode de Linkage: Single-Linkage

La distance entre deux clusters A et B est le **minimum** des distances entre leurs √©l√©ments:

```
distance(A, B) = min(distance(fichier_i, fichier_j))
                 pour tout fichier_i ‚àà A, fichier_j ‚àà B
```

---

## ‚öôÔ∏è Param√®tres du Syst√®me (Article de Recherche)

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| **Taille de bloc HDFS** | 128 MB | Taille standard d'un bloc HDFS |
| **Seuil (threshold)** | 0.75 (75%) | Pourcentage de la taille de bloc |
| **Taille max petit fichier** | 96 MB | 75% √ó 128 MB |
| **Taille max cluster** | 128 MB | Contrainte de fusion |
| **M√©thode de linkage** | Single-Linkage | Distance minimale |

---

## üèóÔ∏è Structure du Projet

```
projet_hdfs/
‚îÇ
‚îú‚îÄ‚îÄ models/                      # Classes de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ small_file.py           # Classe SmallFile (nom, taille)
‚îÇ   ‚îî‚îÄ‚îÄ cluster.py              # Classe Cluster (groupe de fichiers)
‚îÇ
‚îú‚îÄ‚îÄ core/                        # Algorithmes principaux
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ distance_matrix.py      # Calcul de la matrice de distance
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py           # Clustering hi√©rarchique agglom√©ratif
‚îÇ   ‚îî‚îÄ‚îÄ merger.py               # Fusion physique des fichiers
‚îÇ
‚îú‚îÄ‚îÄ data_io/                     # Entr√©es/Sorties
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ file_generator.py       # G√©n√©ration de fichiers de test
‚îÇ   ‚îî‚îÄ‚îÄ metadata_writer.py      # √âcriture des m√©tadonn√©es JSON
‚îÇ
‚îú‚îÄ‚îÄ output/                      # Dossier de sortie (cr√©√© automatiquement)
‚îÇ   ‚îú‚îÄ‚îÄ cluster_*.bin           # Fichiers fusionn√©s
‚îÇ   ‚îú‚îÄ‚îÄ cluster_*_metadata.json # M√©tadonn√©es individuelles
‚îÇ   ‚îú‚îÄ‚îÄ clusters_summary.json   # R√©sum√© de tous les clusters
‚îÇ   ‚îî‚îÄ‚îÄ detailed_report.txt     # Rapport d√©taill√©
‚îÇ
‚îú‚îÄ‚îÄ main.py                      # Point d'entr√©e du programme
‚îú‚îÄ‚îÄ README.md                    # Ce fichier
‚îî‚îÄ‚îÄ requirements.txt             # D√©pendances Python
```

---

## üì¶ Installation

### Pr√©requis

- Python 3.7 ou sup√©rieur
- Aucune biblioth√®que externe requise (biblioth√®que standard uniquement)

### Installation

```bash
# Cloner ou t√©l√©charger le projet
cd projet_hdfs

# Optionnel: cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les d√©pendances (aucune pour ce projet)
pip install -r requirements.txt
```

---

## üöÄ Utilisation

### Ex√©cution du Programme

```bash
python main.py
```

### Modes d'Ex√©cution

Le programme propose plusieurs modes:

1. **Exemple 1**: Sc√©nario r√©aliste avec fichiers mixtes
2. **Exemple 2**: Liste personnalis√©e de fichiers
3. **Exemple 3**: Probl√®me des petits fichiers
4. **Mode interactif**: Configuration personnalis√©e
5. **Tous les exemples**: Ex√©cution s√©quentielle

---

## üìä Exemple d'Entr√©e

### G√©n√©ration Automatique

```python
from io.file_generator import FileGenerator

generator = FileGenerator()
files = generator.generate_realistic_scenario("mixed")
```

G√©n√®re un ensemble de fichiers comme:
```
file_0001.dat (5.00 MB)
file_0002.dat (10.00 MB)
file_0003.dat (20.00 MB)
...
```

### Cr√©ation Manuelle

```python
from models.small_file import SmallFile

files = [
    SmallFile("doc1.txt", 10.0),
    SmallFile("img1.jpg", 25.0),
    SmallFile("video.mp4", 45.0),
]
```

---

## üì§ Exemple de Sortie

### Fichiers Fusionn√©s

```
output/
‚îú‚îÄ‚îÄ cluster_1.bin        # Cluster 1 (96.5 MB, 8 fichiers)
‚îú‚îÄ‚îÄ cluster_2.bin        # Cluster 2 (120.0 MB, 5 fichiers)
‚îî‚îÄ‚îÄ cluster_3.bin        # Cluster 3 (78.3 MB, 6 fichiers)
```

### M√©tadonn√©es JSON

**`clusters_summary.json`:**
```json
{
  "total_clusters": 3,
  "clusters": [
    {
      "cluster_id": 1,
      "files": ["file_0001.dat", "file_0003.dat", "file_0005.dat"],
      "file_count": 3,
      "size_total_mb": 96.5
    },
    {
      "cluster_id": 2,
      "files": ["file_0002.dat", "file_0004.dat"],
      "file_count": 2,
      "size_total_mb": 120.0
    }
  ],
  "summary": {
    "total_files": 19,
    "total_size_mb": 294.8,
    "file_reduction_rate_percent": 84.21
  }
}
```

### Rapport D√©taill√©

**`detailed_report.txt`:**
```
================================================================================
RAPPORT DE FUSION DE FICHIERS HDFS
================================================================================

Nombre de fichiers originaux: 19
Nombre de clusters cr√©√©s: 3
Taux de r√©duction: 84.21%

--------------------------------------------------------------------------------
D√âTAILS DES CLUSTERS
--------------------------------------------------------------------------------

Cluster ID: 1
  Nombre de fichiers: 8
  Taille totale: 96.50 MB
  Fichiers:
    - file_0001.dat (5.00 MB)
    - file_0003.dat (10.00 MB)
    ...
```

---

## üéØ Contraintes Techniques

### Impl√©mentation

‚úÖ **Programmation orient√©e objets** stricte  
‚úÖ **Aucune biblioth√®que externe** de clustering (pas scipy, pas scikit-learn)  
‚úÖ **Algorithme manuel** compl√®tement impl√©ment√©  
‚úÖ **M√©thode single-linkage** respect√©e  
‚úÖ **Contrainte de taille** de 128 MB par cluster  
‚úÖ **Distance euclidienne** bas√©e uniquement sur la taille  

### Classes Principales

- `SmallFile`: Repr√©sente un fichier (nom, taille)
- `Cluster`: Groupe de fichiers
- `DistanceMatrix`: Matrice de distance entre clusters
- `AgglomerativeClustering`: Algorithme de clustering
- `FileMerger`: Fusion physique des fichiers
- `MetadataWriter`: √âcriture des m√©tadonn√©es

---

## üìà Performances

### Complexit√©

- **Temps**: O(n¬≥) dans le pire cas
  - n¬≤ paires √† consid√©rer
  - n it√©rations maximum
  
- **Espace**: O(n¬≤) pour la matrice de distance

### Optimisations Possibles

- Utiliser une structure de donn√©es plus efficace (heap)
- Impl√©menter un cache pour les distances
- Parall√©liser le calcul de la matrice

---

## üîß Configuration

### Modifier la Taille Maximale

Dans `main.py`:
```python
clustering = AgglomerativeClustering(max_cluster_size_mb=256.0)  # 256 MB
```

### Changer le R√©pertoire de Sortie

```python
merger = FileMerger(output_dir="mes_resultats")
metadata_writer = MetadataWriter(output_dir="mes_resultats")
```

---

## üìù Exemples d'Utilisation Avanc√©e

### Utiliser l'API Programmatique

```python
from models.small_file import SmallFile
from core.clustering import AgglomerativeClustering
from core.merger import FileMerger
from data_io.metadata_writer import MetadataWriter

# 1. Cr√©er des fichiers
files = [
    SmallFile("data1.csv", 15.0),
    SmallFile("data2.csv", 18.0),
    SmallFile("img.jpg", 30.0),
]

# 2. Clustering
clustering = AgglomerativeClustering(max_cluster_size_mb=128.0)
clusters = clustering.fit(files)

# 3. Fusion
merger = FileMerger(output_dir="output")
merger.merge_all_clusters(clusters)

# 4. M√©tadonn√©es
writer = MetadataWriter(output_dir="output")
writer.write_all_metadata(clusters)
```

### G√©n√©rer un Sc√©nario Personnalis√©

```python
from data_io.file_generator import FileGenerator

generator = FileGenerator()

# Distribution personnalis√©e
distribution = {
    5: 20,   # 20 fichiers de 5 MB
    15: 10,  # 10 fichiers de 15 MB
    30: 5,   # 5 fichiers de 30 MB
}

files = generator.generate_with_distribution(distribution)
```

---

## üß™ Tests

### V√©rifier le Fonctionnement

Ex√©cutez le programme avec l'exemple 1:
```bash
python main.py
# Choisir: 1
```

V√©rifiez les fichiers dans le dossier `output/`:
- Fichiers `.bin` (fichiers fusionn√©s)
- Fichiers `.json` (m√©tadonn√©es)
- Fichiers `.txt` (rapports)

---

## ü§ù Contribution

### Architecture du Code

Le code est organis√© en modules ind√©pendants:
- **models**: Structures de donn√©es
- **core**: Algorithmes
- **data_io**: G√©n√©ration et √©criture

### Ajouter une Fonctionnalit√©

1. Identifier le module appropri√©
2. Cr√©er une nouvelle classe ou m√©thode
3. Documenter avec des docstrings
4. Mettre √† jour le `README.md`

---

## üìö R√©f√©rences

### HDFS Small Files Problem

- [Apache Hadoop Documentation](https://hadoop.apache.org/)
- [The Small Files Problem in HDFS](https://blog.cloudera.com/the-small-files-problem/)

### Clustering Hi√©rarchique

- Algorithme: Agglomerative Hierarchical Clustering
- Linkage: Single-linkage (nearest neighbor)
- Distance: Euclidienne (taille des fichiers)

---

## üìÑ Licence

Ce projet est fourni √† des fins √©ducatives.

---

## üë®‚Äçüíª Auteur

Projet acad√©mique - M1 Data Science  
Date: Novembre 2025

---

## üéì Notes P√©dagogiques

### Concepts Illustr√©s

1. **Programmation orient√©e objets**
   - Encapsulation
   - H√©ritage conceptuel
   - Polymorphisme

2. **Algorithmes de clustering**
   - Clustering hi√©rarchique
   - Matrice de distance
   - M√©thodes de linkage

3. **Syst√®mes distribu√©s**
   - Probl√®me des petits fichiers
   - Optimisation du stockage
   - Gestion des m√©tadonn√©es

### Points Cl√©s

- ‚úÖ Aucune biblioth√®que externe de ML
- ‚úÖ Impl√©mentation compl√®te de l'algorithme
- ‚úÖ Code comment√© et document√©
- ‚úÖ Architecture modulaire
- ‚úÖ Gestion propre des fichiers

---

**Bon clustering ! üöÄ**
