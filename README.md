# Projet HDFS - Fusion de Petits Fichiers

## üìã Description du Projet

Impl√©mentation de l'algorithme de fusion de petits fichiers bas√© sur l'article de recherche :

> **"Merging Small Files Based on Agglomerative Hierarchical Clustering on HDFS for Cloud Storage"**  
> *Khin Su Su Wai, Julia Myint, Tin Tin Yee - University of Information Technology, Yangon, Myanmar*

### Le Probl√®me des Petits Fichiers (Small Files Problem)

Selon l'article (Section 1):
- *"The consumption of memory in NameNode is decided by the number of files stored in HDFS"*
- *"Each file requires 150 bytes of memory space to store metadata in NameNode"*
- *"When the large number of small files is stored, HDFS is inefficient because of high memory usage"*

### La Solution Propos√©e

Ce programme impl√©mente l'**Algorithm 1: Small Files Merging Algorithm** de l'article :
- Clustering hi√©rarchique agglom√©ratif avec **single-linkage**
- Distance **euclidienne** bas√©e sur la taille des fichiers
- Contrainte de taille : clusters ‚â§ **128 MB** (taille de bloc HDFS)

---

## üßÆ Algorithm 1 - Small Files Merging Algorithm

### Impl√©mentation de l'article (Section 4.1)

```
Input:  Small files S = {F‚ÇÅ, F‚ÇÇ, F‚ÇÉ, ..., F‚Çô}
Output: Cluster hierarchies C = {C‚ÇÅ, C‚ÇÇ, ..., C‚Çò}

Method:
(1-5)  Pour chaque paire (F·µ¢, F‚±º): Calculer De(F·µ¢, F‚±º) = |size_i - size_j|
(6)    Cr√©er la matrice de distance (C, S, De)
(7)    C = {{F} | F ‚àà S}  // Chaque fichier = 1 cluster
(8)    While sizeOfEachCluster |C| < 128MB Do
(9)        {C, C'} = min De(F·µ¢, F‚±º)  // Single-linkage
(10)       If (|C| + |C'|) ‚â§ 128MB Then
(11)           C = ({C} ‚à™ {C'})  // Fusionner
(13)       Update distance matrix (C, S, De)
(14)   End while
(15)   Return (C)
```

### Distance Euclidienne (Section 3)

Selon l'article : *"The Euclidean distance measure is used to cluster the small files"*

```
De(F·µ¢, F‚±º) = |size_i - size_j|
```

Exemple de l'article (Section 4.2):
- d(F‚ÇÅ, F‚ÇÇ) = |40 - 10| = 30 MB
- d(F‚ÇÅ, F‚ÇÉ) = |40 - 50| = 10 MB

### M√©thode Single-Linkage (Section 3)

*"The single-linkage clustering is the minimum distance between elements of each cluster"*

```
distance(Cluster_A, Cluster_B) = min{ De(F·µ¢, F‚±º) | F·µ¢ ‚àà A, F‚±º ‚àà B }
```

---

## ‚öôÔ∏è Configuration HDFS (Selon Article)

| Param√®tre | Valeur | R√©f√©rence Article |
|-----------|--------|-------------------|
| **Taille de bloc HDFS** | 128 MB | Section 1: *"Each file is split into several blocks with the size of 128MB"* |
| **Seuil petits fichiers** | 75% (96 MB) | Section 4: *"The default threshold is set to (0.75) 75% of default block size"* |
| **M√©tadonn√©es/fichier** | 150 bytes | Section 1: *"Each file requires 150 bytes of memory space"* |
| **Taille max cluster** | 128 MB | Section 4: *"The size of cluster should less than or equal to default block size"* |
| **Linkage** | Single | Section 3: *"Single-linkage clustering is the minimum distance"* |

---

## üèóÔ∏è Structure du Projet

```
projet_hdfs/
‚îÇ
‚îú‚îÄ‚îÄ models/                          # Mod√®les de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ small_file.py               # Classe SmallFile - Repr√©sente F·µ¢ ‚àà S
‚îÇ   ‚îî‚îÄ‚îÄ cluster.py                  # Classe Cluster - Repr√©sente C·µ¢ ‚àà C
‚îÇ
‚îú‚îÄ‚îÄ core/                            # Algorithmes (Algorithm 1)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py               # AgglomerativeClustering - Lignes 1-15
‚îÇ   ‚îú‚îÄ‚îÄ distance_matrix.py          # DistanceMatrix - De(F·µ¢, F‚±º)
‚îÇ   ‚îú‚îÄ‚îÄ dendrogram.py               # Dendrogram - Arbre hi√©rarchique
‚îÇ   ‚îú‚îÄ‚îÄ merger.py                   # FileMerger - Fusion physique
‚îÇ   ‚îú‚îÄ‚îÄ namenode_memory.py          # NameNodeMemory - Simulation m√©moire
‚îÇ   ‚îî‚îÄ‚îÄ file_index.py               # FileIndex - Index de r√©cup√©ration
‚îÇ
‚îú‚îÄ‚îÄ data_io/                         # Entr√©es/Sorties
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ file_generator.py           # G√©n√©ration de fichiers de test
‚îÇ   ‚îî‚îÄ‚îÄ metadata_writer.py          # √âcriture des m√©tadonn√©es JSON
‚îÇ
‚îú‚îÄ‚îÄ output/                          # R√©sultats (cr√©√© automatiquement)
‚îÇ   ‚îú‚îÄ‚îÄ cluster_*.bin               # Fichiers fusionn√©s
‚îÇ   ‚îú‚îÄ‚îÄ cluster_*_metadata.json     # M√©tadonn√©es par cluster
‚îÇ   ‚îú‚îÄ‚îÄ example*_clusters.json      # R√©sum√© des clusters
‚îÇ   ‚îî‚îÄ‚îÄ example*_report.txt         # Rapports d√©taill√©s
‚îÇ
‚îú‚îÄ‚îÄ main.py                          # Point d'entr√©e du programme
‚îú‚îÄ‚îÄ requirements.txt                 # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                        # Ce fichier
```

---

## üì¶ Installation

### Pr√©requis

- Python 3.7 ou sup√©rieur
- Aucune biblioth√®que externe requise (biblioth√®que standard uniquement)

### Installation

```bash
# Cloner ou t√©l√©charger le projet
cd projet_hdfs

# Optionnel: cr√©er un environnement virtuel
python -m venv venv
venv\Scripts\activate     # Windows
# ou
source venv/bin/activate  # Linux/Mac

# Installer les d√©pendances (aucune externe)
pip install -r requirements.txt
```

---

## üöÄ Utilisation

### Ex√©cution du Programme

```bash
python main.py
```

### Menu Principal

```
Choisissez un mode d'ex√©cution:

  1. Exemple 1: Sc√©nario r√©aliste (fichiers mixtes)
  2. Exemple 2: Liste personnalis√©e de fichiers
  3. Exemple 3: Probl√®me des petits fichiers
  4. Mode interactif
  5. Ex√©cuter tous les exemples
  0. Quitter
```

### Exemple de Sortie Console

```
============================================================
D√âMARRAGE DU CLUSTERING HI√âRARCHIQUE AGGLOM√âRATIF
============================================================
Configuration HDFS (selon article):
  - Taille de bloc: 128.0 MB
  - Seuil petits fichiers: 75% = 96.0 MB
  - Taille max cluster: 128.0 MB

[ALGORITHM 1 - Lignes 1-6] Calcul de la matrice de distance euclidienne...

[Ligne 9] Single-linkage: min distance = 0.00 MB
[Ligne 10] Contrainte: 5.00 + 5.00 = 10.00 MB <= 128.0 MB
[Ligne 11] Fusion: C = (C1 ‚à™ C2)
-> Nouveau cluster C15 cr√©√© (10.00 MB, 2 fichiers)
[Ligne 13] Mise √† jour matrice de distance
```

---

## üìä Classes Principales

### SmallFile (models/small_file.py)

Repr√©sente un fichier F·µ¢ dans l'ensemble S = {F‚ÇÅ, F‚ÇÇ, ..., F‚Çô}

```python
from models.small_file import SmallFile, HDFS_BLOCK_SIZE_MB, SMALL_FILE_THRESHOLD

file = SmallFile("document.txt", 25.0)  # 25 MB
print(file.size_mb)  # 25.0
```

**Constantes d√©finies:**
- `HDFS_BLOCK_SIZE_MB = 128.0`
- `SMALL_FILE_THRESHOLD = 0.75`
- `SMALL_FILE_MAX_SIZE_MB = 96.0`

### Cluster (models/cluster.py)

Repr√©sente un cluster C·µ¢ dans l'ensemble C = {C‚ÇÅ, C‚ÇÇ, ..., C‚Çò}

```python
from models.cluster import Cluster

cluster = Cluster([file1, file2])
print(cluster.get_total_size())  # Taille totale en MB
print(cluster.can_merge_with(other_cluster, max_size_mb=128.0))
```

### AgglomerativeClustering (core/clustering.py)

Impl√©mente l'Algorithm 1 de l'article (lignes 1-15)

```python
from core.clustering import AgglomerativeClustering

clustering = AgglomerativeClustering(max_cluster_size_mb=128.0)
clusters = clustering.fit(files)  # Ex√©cute l'Algorithm 1
clustering.print_algorithm_steps()  # Affiche le journal d'ex√©cution
```

### DistanceMatrix (core/distance_matrix.py)

Calcule et maintient la matrice De(F·µ¢, F‚±º)

```python
from core.distance_matrix import DistanceMatrix

matrix = DistanceMatrix(clusters)
i, j, distance = matrix.find_closest_pair()  # Ligne 9: single-linkage
matrix.merge_clusters(i, j)  # Lignes 11 et 13
```

### Dendrogram (core/dendrogram.py)

Construit l'arbre hi√©rarchique des fusions

```python
clustering.dendrogram.print_tree()
clustering.dendrogram.print_merge_history()
```

### NameNodeMemory (core/namenode_memory.py)

Simule la consommation m√©moire du NameNode (150 bytes/entr√©e)

```python
from core.namenode_memory import NameNodeMemory

namenode = NameNodeMemory()
report = namenode.get_detailed_report(files, clusters)
print(report)
```

**Exemple de sortie (selon article Section 5):**
```
ANALYSE M√âMOIRE NAMENODE
  Fichiers originaux: 15 ‚Üí 2250 bytes
  Clusters fusionn√©s: 3 ‚Üí 450 bytes
  √âconomie: 1800 bytes (80.00%)
```

### FileIndex (core/file_index.py)

Index pour r√©cup√©rer les fichiers depuis les clusters fusionn√©s

```python
from core.file_index import FileIndex

index = FileIndex()
index.build_index(clusters)
location = index.get_file_location("document.txt")
# Retourne: (cluster_id, offset, size)
```

---

## üì§ Fichiers de Sortie

### Fichiers Fusionn√©s (*.bin)

Fichiers binaires contenant les donn√©es fusionn√©es de chaque cluster.

### M√©tadonn√©es JSON

**`example1_clusters.json`:**
```json
{
  "total_clusters": 3,
  "clusters": [
    {
      "cluster_id": 24,
      "files": ["file_0001.dat", "file_0003.dat", "file_0005.dat"],
      "file_count": 3,
      "size_total_mb": 96.5
    }
  ],
  "summary": {
    "total_files": 15,
    "total_size_mb": 294.8,
    "memory_reduction_percent": 80.0
  }
}
```

### Rapports D√©taill√©s

**`example1_report.txt`:**
```
================================================================================
RAPPORT DE FUSION DE FICHIERS HDFS
================================================================================

Configuration (selon article):
  - Taille de bloc HDFS: 128 MB
  - Seuil petits fichiers: 75% = 96 MB
  - M√©tadonn√©es par entr√©e: 150 bytes

R√©sultats:
  - Fichiers originaux: 15
  - Clusters cr√©√©s: 3
  - R√©duction m√©moire: 80.00%
```

---

## üìà √âvaluation (Section 5 de l'article)

### Comparaison M√©moire NameNode

| Sc√©nario | Fichiers | Original HDFS | Proposed Approach | R√©duction |
|----------|----------|---------------|-------------------|-----------|
| Exemple 1 | 6 fichiers | 900 bytes | 300 bytes (2 clusters) | 66.7% |
| Exemple 2 | 8 fichiers | 1200 bytes | 450 bytes (3 clusters) | 62.5% |
| Exemple 3 | 11 fichiers | 1650 bytes | 150 bytes (1 cluster) | 90.9% |

*Valeurs tir√©es de l'article - Figure 3: NameNode Memory Consumption*

-----

## üß™ Tests

### Ex√©cuter l'Exemple 1

```bash
python main.py
# Choisir: 1
```

### V√©rifier les R√©sultats

```bash
# Fichiers cr√©√©s dans output/
ls output/
# cluster_*.bin, cluster_*_metadata.json, example1_*.json, example1_*.txt
```

---

## üìö R√©f√©rences

### Article de Recherche

> Khin Su Su Wai, Julia Myint, Tin Tin Yee. "Merging Small Files Based on Agglomerative Hierarchical Clustering on HDFS for Cloud Storage". University of Information Technology, Yangon, Myanmar.

### Concepts Cl√©s

- **HDFS**: Hadoop Distributed File System
- **NameNode**: Gestionnaire de m√©tadonn√©es HDFS
- **Small Files Problem**: Surcharge m√©moire due aux nombreux petits fichiers
- **Agglomerative Clustering**: Clustering bottom-up
- **Single-Linkage**: Distance = minimum entre √©l√©ments

---

## üë®‚Äçüíª Auteur

Projet acad√©mique - M1 Data Science  
D√©cembre 2025

---

**Bon clustering ! üöÄ**
